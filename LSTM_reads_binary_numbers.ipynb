{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.ops import ctc_ops as ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:\n",
      "  [ 1.  0.  0.  0.  0.  1.  1.  1.  0.]\n",
      "  [ 0.  1.  1.  1.  0.  0.  0.  0.  0.]\n",
      "transcription:\n",
      "  [8 7]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "max_y_seq_len = 10\n",
    "\n",
    "def gen_data(max_y_seq_len, num_classes):\n",
    "    \"\"\" generate sequence\n",
    "    \n",
    "    sequences are binary representation of the class number (without any noise)\n",
    "    \n",
    "    example: \n",
    "        sequence:\n",
    "          [ 0.  0.  1.  0.  1.  0.  1.  1.  0.]\n",
    "          [ 1.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
    "        transcription:\n",
    "          [0 5 3]\n",
    "\n",
    "    Args:\n",
    "        max_y_seq_len: max length of the sequence\n",
    "        num_classes: number of classes\n",
    "    Returns: \n",
    "        sequence and transcription\n",
    "    \"\"\"\n",
    "    seq_len = np.random.randint(1,max_y_seq_len)\n",
    "    y = np.random.randint(num_classes-1,size = [seq_len])\n",
    "    x = np.zeros([sum([len(bin(tmp)[2:]) for tmp in y])+seq_len,2])\n",
    "    pos = 0\n",
    "    for i in range(seq_len):\n",
    "        b = bin(y[i])[2:]\n",
    "        for j,c in enumerate(b):\n",
    "            x[pos+j,0] = int(c)\n",
    "            x[pos+j,1] = 1-int(c)\n",
    "        pos += len(b)+1\n",
    "    return x,y\n",
    "\n",
    "example = gen_data(4, num_classes)\n",
    "print(\"sequence:\")\n",
    "print(\" \",example[0][:,0])\n",
    "print(\" \",example[0][:,1])\n",
    "print(\"transcription:\")\n",
    "print(\" \",example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_x_len = max_y_seq_len*int(np.ceil(np.log(num_classes)/np.log(2))+1)\n",
    "def new_batch(batch_size):\n",
    "    x = np.zeros([batch_size, max_x_len, 2])\n",
    "    y_indices = []\n",
    "    y_values = []\n",
    "    y_shape = []\n",
    "    x_seq_length = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        x_new, y_new = gen_data(max_y_seq_len, num_classes)\n",
    "        x[b][:len(x_new)] = x_new\n",
    "        x_seq_length.append(len(x_new))\n",
    "        for t in range(len(y_new)):\n",
    "            y_indices.append([b,t])\n",
    "            y_values.append(y_new[t])\n",
    "    y_shape = [batch_size, max_y_seq_len]\n",
    "    return np.array(x), x_seq_length, [y_indices,y_values,y_shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_hidden_neuron = 10\n",
    "\n",
    "#input and targets\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, max_x_len, 2])\n",
    "seq_lengths = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "y_truth_indices = tf.placeholder(tf.int64)\n",
    "y_truth_values = tf.placeholder(tf.int32)\n",
    "y_truth_shape = tf.placeholder(tf.int64)\n",
    "y_truth = tf.SparseTensor(y_truth_indices, y_truth_values, y_truth_shape)\n",
    "y_truth_splitted = tf.sparse_split(0, batch_size, y_truth)\n",
    "\n",
    "#LSTM-RNN\n",
    "lstm_cell_fw = rnn_cell.BasicLSTMCell(num_hidden_neuron, \n",
    "    forget_bias=1.0, state_is_tuple = True)\n",
    "lstm_cell_bw = rnn_cell.BasicLSTMCell(num_hidden_neuron, \n",
    "    forget_bias=1.0, state_is_tuple = True)\n",
    "x_transposed = tf.transpose(x, [1, 0, 2])\n",
    "x_reshaped = tf.reshape(x_transposed, [-1, 2])\n",
    "x_list = tf.split(0, max_x_len, x_reshaped)\n",
    "outputs_lstm, states_fw, states_bw = rnn.bidirectional_rnn(\n",
    "    lstm_cell_fw, lstm_cell_bw, x_list, dtype=tf.float32)\n",
    "outputs_lstm_reshaped = [tf.reshape(t, [batch_size, 2 * num_hidden_neuron]) for t in outputs_lstm]\n",
    "W = tf.Variable(tf.truncated_normal([2*num_hidden_neuron,num_classes], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "logits = tf.pack([tf.matmul(t, W) + b for t in outputs_lstm_reshaped])\n",
    "\n",
    "#training\n",
    "loss = tf.reduce_mean(ctc.ctc_loss(logits, y_truth, seq_lengths, ctc_merge_repeated=False))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.003).minimize(loss)\n",
    "\n",
    "#predictions\n",
    "y_predictions_unsplitted = ctc.ctc_beam_search_decoder(logits, seq_lengths, merge_repeated=False)[0][0]\n",
    "error = tf.reduce_sum(tf.edit_distance(tf.to_int32(y_predictions_unsplitted), \n",
    "    y_truth, normalize=False)) / tf.to_float(tf.size(y_truth.values))\n",
    "y_predictions = tf.sparse_split(0, batch_size, y_predictions_unsplitted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.044898 (loss = 29.864012)\n",
      "   [0 3] ->\n",
      "   [7 5]\n",
      "Epoch 100: 0.954545 (loss = 11.147442)\n",
      "   [3 2] ->\n",
      "   []\n",
      "Epoch 200: 0.951111 (loss = 11.151478)\n",
      "   [4 4 3 3 6] ->\n",
      "   []\n",
      "Epoch 300: 0.863469 (loss = 12.339660)\n",
      "   [6 6 5 8 7] ->\n",
      "   [6]\n",
      "Epoch 400: 0.862069 (loss = 10.999942)\n",
      "   [7 5 1 3 4 8 1 0] ->\n",
      "   [7 8]\n",
      "Epoch 500: 0.793991 (loss = 9.169186)\n",
      "   [2 2 4] ->\n",
      "   [4]\n",
      "Epoch 600: 0.194853 (loss = 7.047174)\n",
      "   [0 6 4 7] ->\n",
      "   [0 6 4 7]\n",
      "Epoch 700: 0.000000 (loss = 3.053547)\n",
      "   [4] ->\n",
      "   [4]\n",
      "Epoch 800: 0.000000 (loss = 1.541650)\n",
      "   [8 2 5] ->\n",
      "   [8 2 5]\n",
      "Epoch 900: 0.000000 (loss = 0.885364)\n",
      "   [8 2] ->\n",
      "   [8 2]\n",
      "Epoch 1000: 0.000000 (loss = 0.516311)\n",
      "   [1 1 7 6] ->\n",
      "   [1 1 7 6]\n",
      "Epoch 1100: 0.000000 (loss = 0.356968)\n",
      "   [1 2 7 8 4 1 5 6] ->\n",
      "   [1 2 7 8 4 1 5 6]\n",
      "Epoch 1200: 0.000000 (loss = 0.239811)\n",
      "   [7 7 3 6] ->\n",
      "   [7 7 3 6]\n",
      "Epoch 1300: 0.000000 (loss = 0.191230)\n",
      "   [2 6 1 1 8 6 8 6] ->\n",
      "   [2 6 1 1 8 6 8 6]\n",
      "Epoch 1400: 0.000000 (loss = 0.161497)\n",
      "   [2 8 4] ->\n",
      "   [2 8 4]\n",
      "Epoch 1500: 0.000000 (loss = 0.123336)\n",
      "   [5 2 1 4 7 3] ->\n",
      "   [5 2 1 4 7 3]\n",
      "Epoch 1600: 0.000000 (loss = 0.108276)\n",
      "   [4 5 0] ->\n",
      "   [4 5 0]\n",
      "Epoch 1700: 0.000000 (loss = 0.083086)\n",
      "   [0 1 7] ->\n",
      "   [0 1 7]\n",
      "Epoch 1800: 0.000000 (loss = 0.086476)\n",
      "   [8 7 1 1 2 8 8 3] ->\n",
      "   [8 7 1 1 2 8 8 3]\n",
      "Epoch 1900: 0.000000 (loss = 0.068214)\n",
      "   [1 5 2 7 2] ->\n",
      "   [1 5 2 7 2]\n",
      "Epoch 2000: 0.000000 (loss = 0.054765)\n",
      "   [4 2 1 5 5 5 1] ->\n",
      "   [4 2 1 5 5 5 1]\n",
      "Epoch 2100: 0.000000 (loss = 0.042873)\n",
      "   [2 2 1 6 3 1] ->\n",
      "   [2 2 1 6 3 1]\n",
      "Epoch 2200: 0.000000 (loss = 0.045620)\n",
      "   [2 6 5] ->\n",
      "   [2 6 5]\n",
      "Epoch 2300: 0.000000 (loss = 0.047959)\n",
      "   [3 0 3 7 1 3 8 4 6] ->\n",
      "   [3 0 3 7 1 3 8 4 6]\n",
      "Epoch 2400: 0.000000 (loss = 0.032550)\n",
      "   [1 1 1] ->\n",
      "   [1 1 1]\n",
      "Epoch 2500: 0.000000 (loss = 0.031086)\n",
      "   [6 5 3 0 2] ->\n",
      "   [6 5 3 0 2]\n",
      "Epoch 2600: 0.000000 (loss = 0.027046)\n",
      "   [3 3 0 5] ->\n",
      "   [3 3 0 5]\n",
      "Epoch 2700: 0.000000 (loss = 0.027413)\n",
      "   [0 3 4 6 5 8 6 2 3] ->\n",
      "   [0 3 4 6 5 8 6 2 3]\n",
      "Epoch 2800: 0.000000 (loss = 0.025613)\n",
      "   [5 5 5 6 4 0 7] ->\n",
      "   [5 5 5 6 4 0 7]\n",
      "Epoch 2900: 0.000000 (loss = 0.023457)\n",
      "   [6 7 6 0] ->\n",
      "   [6 7 6 0]\n",
      "Epoch 3000: 0.000000 (loss = 0.018961)\n",
      "   [4 4 0 7] ->\n",
      "   [4 4 0 7]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch in range(4000):\n",
    "        x_batch,seq_lengths_batch,y_batch = new_batch(batch_size)\n",
    "        feed_dict = {x: x_batch, seq_lengths: seq_lengths_batch,\n",
    "            y_truth_indices: y_batch[0], y_truth_values: y_batch[1], y_truth_shape: y_batch[2]}\n",
    "        if epoch % 100 != 0:\n",
    "            train_step.run(feed_dict = feed_dict)\n",
    "        else: #evaluate on (not for training used) batch\n",
    "            loss_batch, error_batch, y_predictions_batch, y_truth_batch = \\\n",
    "                sess.run([loss, error, y_predictions, y_truth_splitted],feed_dict = feed_dict)\n",
    "            print('Epoch %i: %f (loss = %f)' %(epoch, error_batch, loss_batch))\n",
    "            print('  ', y_truth_batch[13].values, \"->\")\n",
    "            print('  ', y_predictions_batch[13].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
